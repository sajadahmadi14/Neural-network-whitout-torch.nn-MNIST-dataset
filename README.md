# Neural-network-whitout-torch.nn-MNIST-dataset

This GitHub repository contains code for implementing a neural network from scratch, without relying on torch.nn or any pre-built cost functions. The neural network is specifically trained on the MNIST dataset, which is a commonly-used dataset in machine learning for image classification tasks.

The code in this repository is designed to showcase how a neural network can be implemented and trained without relying on pre-built functions, allowing for a deeper understanding of how neural networks work under the hood. By building the model from scratch and using custom-defined cost functions, users can gain a better understanding of the mathematical concepts that underlie neural network training.

To use this repository, users can clone it and run the provided Python files on their local machine. The code includes all the necessary functions for implementing a neural network, including functions for forward and backward propagation, weight initialization, and gradient descent optimization. Additionally, the code includes all the necessary preprocessing steps for working with the MNIST dataset, such as loading the data into memory and normalizing the pixel values.

By training the neural network on the MNIST dataset, users can see firsthand how the model learns to recognize digits based on their pixel values. The repository also includes metrics for tracking the accuracy of the model during both training and testing, ensuring that the model is not overfitting to the training data.

Overall, this GitHub repository provides a valuable resource for anyone interested in deepening their understanding of neural networks and how they can be implemented from scratch.
